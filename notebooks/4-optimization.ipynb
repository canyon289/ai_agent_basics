{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Optimization  \n",
    "\n",
    "Now that we have a structured evaluation process, itâ€™s time to **optimize and refine** our system.  \n",
    "\n",
    "In this part, weâ€™ll:  \n",
    "- **Test different models** to compare their strengths.  \n",
    "- **Analyze trade-offs** between speed, performance, and cost.  \n",
    "- **Make informed decisions** about improvements and potential deployment.  \n",
    "\n",
    "This is where we shift from a prototype to something more **robust and practical**. Whether you're thinking about deploying an LLM system or simply improving local performance, this step will help you make data-driven optimizations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there's no free lunch. \n",
    "Let's change some inference parameters and see if we can get a speedup,\n",
    "but use our evals to understand what the tradeoff is.\n",
    "\n",
    "## Steps\n",
    "* Run inference but this time focus on engineering metrics, in our case we care about clock time.\n",
    "  * Measure 10 responses using the an `gemma2:2b-instruct-fp16` model, then repeat with a `gemma2:2b-instruct-q2_K` model\n",
    "  * Measure the total generation time using python.\n",
    "  * Think about a confounder for generation time, something that strongly effects it. Use some basic mathematics to account for this.\n",
    "* Before running evals, what is different about these models?\n",
    "  * Hint: Google quantization, look at the model size on Ollama, think about what is expensive during inference.\n",
    "* After runing evals\n",
    "  * What do you notice is different?\n",
    "  * What about the quality of the responses? How does that change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "**A Journey Through Time: The History and Achievements of North Melbourne Kangaroos**\\n\\nThe North Melbourne Kangaroos, affectionately known as the Magpies prior to their 1983 rebranding, are a storied team with a rich tapestry of history. Nestled in Ballarat, Victoria, these kangaroos have been the pride of the region since their inception in 1924. This article delves into their remarkable journey, from their early days as a minor league side to becoming a significant force in Australian rugby union.\\n\\n**The Founding of the Kangaroos**\\n\\nNorth Melbourne Kangaroos were born on November 6, 1924, when the North Melbourne Rugby Union Club was established. The club\\'s name change to Kangaroos occurred after their 1983 merger with Torquay United, a move that reflected their transition from being based in Melbourne to Ballarat.\\n\\n**A Legacy of Excellence**\\n\\nThe Kangaroos\\' history is marked by excellence and resilience. Over the decades, they\\'ve claimed multiple State Titles, showcasing their dominance on the field. Notable players such as David Nasmith and Adam Droms have been luminaries of the team, with Nasmith holding the distinction of being one of the youngest players to grace the field in 1958 and still active today.\\n\\n**Current Stance in the League**\\n\\nSince their move to Ballarat in 2019, the North Melbourne Kangaroos have become a prominent side in the Victorian top-tier competition. Their transition was met with both anticipation and nostalgia from their old fans, who cherished the memories of playing at Belmore Ground, the club\\'s historic home.\\n\\n**Community Connection**\\n\\nThe kangaroos are deeply rooted in the Ballarat community. Their branding includes the iconic \"Kangaroo\" logo, symbolizing their connection to the local wildlife. The team has also embraced community involvement, with players and staff engaging actively in local initiatives, fostering a strong bond between the club and its surroundings.\\n\\n**Engaging with Fans**\\n\\nIn keeping with rugby union traditions, the Kangaroos embrace close-knit relationships with their fans. Whether through match days or annual events, the team consistently demonstrates a commitment to community spirit, further cementing its identity as a local institution.\\n\\n**Looking Ahead**\\n\\nThe future of North Melbourne Kangaroos looks promising. With a focus on developing young talent and maintaining high standards, the club is poised for continued success. Fans can expect an exciting season ahead, with the kangaroos aiming to uphold their legacy while exploring new horizons in the league.\\n\\nIn conclusion, the North Melbourne Kangaroos represent more than just a rugby team; they are a symbol of pride for Ballarat and a testament to the enduring spirit of sportsmanship. As they continue to navigate the challenges and opportunities of modern rugby union, one can look forward to many thrilling matches and memorable moments in their storied history.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gemma2:2b'\n",
    "\n",
    "def single_turn_with_time(prompt, model):\n",
    "    start_time = time.time()\n",
    "    response: ChatResponse = chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "      },\n",
    "    ])\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    return response.message.content.strip(), total_time\n",
    "\n",
    "\n",
    "prompt = f\"Is this about an australian or american team?, print no other words: {article}?\"\n",
    "single_turn_with_time(prompt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gemma2:2b-instruct-fp16'\n",
    "\n",
    "for i in range(10):\n",
    "    prompt = f\"Is this about an australian or american team?, print no other words: {article}?\"\n",
    "    print(\"Response {0}: Seconds {1}\".format(*single_turn_with_time(prompt, model=model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gemma2:2b-instruct-fp16'\n",
    "\n",
    "for i in range(10):\n",
    "    prompt = f\"Is this about an australian or american team?, print no other words: {article}?\"\n",
    "    response, wall_time = single_turn_with_time(prompt, model=model)\n",
    "    character_per_second = len(response) / wall_time\n",
    "    print(\"Response {0}: Characters per second {1}\".format(response, character_per_second))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized model\n",
    "Let's try the quantized model. See if there's anything you notice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gemma2:2b-instruct-q2_K'\n",
    "\n",
    "for i in range(10):\n",
    "    prompt = f\"Is this about an australian or american team?, print no other words: {article}?\"\n",
    "    print(\"Response {0}: Seconds {1}\".format(*single_turn_with_time(prompt, model=model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gemma2:2b-instruct-q2_K'\n",
    "\n",
    "for i in range(10):\n",
    "    prompt = f\"Is this about an australian or american team?, print no other words: {article}?\"\n",
    "    response, wall_time = single_turn_with_time(prompt, model=model)\n",
    "    character_per_second = len(response) / wall_time\n",
    "    print(\"Response {0}: Characters per second {1}\".format(response, character_per_second))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "* What is the difference in tokens per second?\n",
    "* What about the quality of the answer?\n",
    "* Is this tradeoff worth it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we pick these models?\n",
    "In this exercise we want to highlight how drastically inference speed can change **even with the same model size**.\n",
    "For this exercise we picked the highest precision \"best quality\" weights and are comparing it to the \"fastest\" but lowest precision weights.\n",
    "\n",
    "Quantization is just one decision to be made when optimizing serving. There are a number of other methods that speed up inference. For example a common one is a [KV Cache](https://medium.com/@joaolages/kv-caching-explained-276520203249). These days most frameworks already include them. See the following blog post and setting in Ollama.\n",
    "\n",
    "\n",
    "Methods can be combined, for example KV Caching and quantization. To see the deep details check out this discussion from the ollama maintainers. https://github.com/ollama/ollama/issues/5091#issuecomment-2476456862\n",
    "\n",
    "For more settings see this [doc](https://github.com/ollama/ollama/blob/main/docs/faq.md)\n",
    "  * os.environ[\"OLLAMA_KV_CACHE_TYPE\"] = \"q4_0\"\n",
    "  * os.environ[\"OLLAMA_KV_CACHE_TYPE\"] = \"FP16\"\n",
    "  * os.environ[\"OLLAMA_FLASH_ATTENTION\"] = \"0\"kv cach\n",
    "\n",
    "For more on efficient serving see the [GenAI Guidebook references](https://ravinkumar.com/GenAiGuidebook/language_models/inference.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Recap: What We Learned  \n",
    "\n",
    "### Inference is as important as training\n",
    "* There are a number of decisions that need to made that balance user experience and cost of running models\n",
    "* Quantization is one such example, especially for open models\n",
    "* There are many others such as prompt caching, speculative decoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
