{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Creating a basic agent\n",
    "\n",
    "Let's go through the fundamentals of creating an agent.\n",
    "Here are the key concepts\n",
    "\n",
    "These are\n",
    "- **Function Definition** what tools the LLM has available to it.  \n",
    "- **Function Call** a response from the model indicating a function call should be made.  \n",
    "- **Function Response** the response we get from the function itself.  \n",
    "\n",
    "And to tie this all together we need an **LLM Framework**\n",
    "\n",
    "## How to do this\n",
    "We're going to implement things one piece at time\n",
    "\n",
    "## Our goal\n",
    "Build the LLM framework from scratch so you can see every piece of how the system worls\n",
    "\n",
    "**We're going to building the entire thing by hand**. \n",
    "\n",
    "Tjhis "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![functioncalling](img/function-calling-overview.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first LLM tool\n",
    "This is an (arbitrary) tool that we want our LLM to use. While this one is simple the tool can be anything. We'll start with this one because its "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city):\n",
    "    logging.info(\"Called weather function %s\", city)\n",
    "    if city.lower() in {\"Austin\", \"Sydney\"}:\n",
    "        return \"sunny\"\n",
    "    else:\n",
    "        return \"cloudy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cloudy'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weather(\"Austin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Chat Bot\n",
    "Let's now start buildlding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello everyone! It's great to be here with you all today! ðŸ˜Š \\n\\n\\n\\nHow's everyone doing?\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "# 12b is better so try and use it first\n",
    "model = 'gemma3:12b'\n",
    "# model = 'gemma3:4b'\n",
    "\n",
    "\n",
    "# Note, the argument model_prompt is specific here\n",
    "def model_call(model_prompt):\n",
    "    \n",
    "    response: ChatResponse = chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': model_prompt,\n",
    "      },\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "user_prompt = \"Say hello to the class\"\n",
    "\n",
    "# Note, the argument user_prompt is specific here\n",
    "model_call(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a system prompt\n",
    "System prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_model_call(system_prompt, user_prompt, print_prompt = False):\n",
    "    combined_prompt = f\"{system_prompt}\\n{user_prompt}\"\n",
    "\n",
    "    if print_prompt:\n",
    "        print(combined_prompt)\n",
    "    \n",
    "    return model_call(combined_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talk like a pirate to everyone. \n",
      " \n",
      "Say hello to the class\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Ahoy, mateys! Shiver me timbers, gather 'round, ye scurvy dogs!\\n\\nAvast there, class! Welcome aboard! May yer studies be fruitful and yer knowledge as vast as the seven seas! Arrr!\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is injected by the LLM application behind the scenes\n",
    "\n",
    "system_prompt = '''Talk like a pirate to everyone. \\n '''\n",
    "augmented_model_call(system_prompt, user_prompt, print_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Call System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You have the following functions available\n",
    " def get_weather(city: str)\n",
    "   \"\"\"Given a city returns the weather for that city\"\"\n",
    "\n",
    " If you call this function return the json [{\"city\": city}] and nothing else\n",
    " otherwise respond normally\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[{\"city\": \"Sydney\"}]\\n```\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"What's the weather in Sydney?\"\n",
    "augmented_model_call(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[{\"city\": \"Austin\"}]\\n```\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"How's the Austin forecast looking today?\"\n",
    "augmented_model_call(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[{\"city\": \"London\"}]\\n```\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"How's the London weather looking today?\"\n",
    "augmented_model_call(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Response\n",
    "Now that we can see our model is aware of the tools we need to build a framework around it to process results.\n",
    "\n",
    "We need a basic condition\n",
    "1. If there is no function call return the response to the user\n",
    "2. If there is a function call then we need to\n",
    "    a. Call the tool to get new information\n",
    "    b. Either return the response back to the user directly, or reinject it back to hte model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 'Austin'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = f\"```json\\n(.*?)\\n``\"\n",
    "\n",
    "def parse_response(model_response):\n",
    "    if tool_call := re.search(pattern, model_response):\n",
    "        return json.loads(tool_call.groups(0)[0])[0]\n",
    "\n",
    "model_response = '```json\\n[{\"city\": \"Austin\"}]\\n```\\n'\n",
    "parse_response(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all the pieces together: Full tool Interaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi class! \\n\\n(And if I were to call `get_weather(\"London\")`, I would return `[{\"city\": \"London\"}]`.)'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_interaction(user_prompt):\n",
    "\n",
    "    system_prompt = '''\n",
    "    You have the following functions available\n",
    "     def get_weather(city: str)\n",
    "       \"\"\"Given a city returns the weather for that city\"\"\n",
    "    \n",
    "     If you call this function return the json [{\"city\": city}] and nothing else\n",
    "     otherwise respond normally\n",
    "    '''\n",
    "\n",
    "    # Get a model response. Right now we don't know if its a function call or chat response\n",
    "    model_response = augmented_model_call(system_prompt, user_prompt)\n",
    "\n",
    "    # Regex to see if we have the json which indicates a function call\n",
    "    function_call_json = parse_response(model_response)\n",
    "\n",
    "    # If it's not a function call return the response\n",
    "    if not function_call_json:\n",
    "        return model_response\n",
    "    \n",
    "    # Since we detect a function call\n",
    "    weather = get_weather(function_call_json[\"city\"])\n",
    "\n",
    "    # We have a choice here\n",
    "    # We could return the weather directly to the user\n",
    "    # But for a nicer experience let's reinject in the LLM to the response\n",
    "    function_response_prompt = f\"The weather in {function_call_json['city']} is {weather}, tell me the weather nicely\"\n",
    "\n",
    "    # We already checked for weather so we don't need to go again\n",
    "    model_response = model_call(function_response_prompt)\n",
    "    return model_response\n",
    "\n",
    "chat_interaction(\"Can you say hi to class?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The sky in London is looking a bit grey and cloudy today! It's a cozy kind of day, perfect for a warm drink and a good book. ðŸ˜Š\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_interaction(\"What's the weather today in London?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The sky in Austin is looking a little cloudy today! It's a cozy, soft kind of day. ðŸ˜Š\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_interaction(\"How are things looking in Austin?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Recap: What We Learned  \n",
    "\n",
    "### Function Calls are nothing magical\n",
    "* We rely on the models \"reasoning\" to understand when to make a function call or not\n",
    "\n",
    "### The model doesn't call a function itself\n",
    "* It's the framework that handles the function call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn: Do this with a real api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(latitude, longitude):\n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LONDON_LATITUDE = 51.5074 # Approximate latitude for London\n",
    "LONDON_LONGITUDE = -0.1278 # Approximate longitude for London (West is negative)\n",
    "get_weather(LONDON_LATITUDE, LONDON_LONGITUDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Call versus Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![functioncalling](img/AlwaysHasBeen.jpg)\n",
    "Source: https://huggingface.co/blog/tiny-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents have a lot of definitions, the simplest however is that they're a model that can call tools in a for loop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
