{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Creating a basic agent\n",
    "\n",
    "Let's go through the fundamentals of creating an agent.\n",
    "Here are the key concepts\n",
    "\n",
    "These are\n",
    "- **Function Definition** what tools the LLM has available to it.  \n",
    "- **Function Call** a response from the model indicating a function call should be made.  \n",
    "- **Function Response** the response we get from the function itself.  \n",
    "\n",
    "And to tie this all together we need an **LLM Framework**\n",
    "\n",
    "## How to do this\n",
    "We're going to implement things one piece at time\n",
    "\n",
    "\n",
    "## Our goal\n",
    "Build the LLM framework from scratch so you can see every piece of how the system worls\n",
    "\n",
    "**Your task is to get a classification of whether this about American football or Australian football**. Ideally we just want the string American or Australian\n",
    "\n",
    "Start prompting the LLM to see what results you get. Use your intuition to iterate to the right behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city):\n",
    "    logging.info(\"Called weather function %s\", city)\n",
    "    if city.lower() in {\"Austin\", \"Sydney\"}:\n",
    "        return \"sunny\"\n",
    "    else:\n",
    "        return \"cloudy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cloudy'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weather(\"Austin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "model = 'gemma3:4b'\n",
    "\n",
    "def model_call(prompt):\n",
    "    \n",
    "    response: ChatResponse = chat(model=model, messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "      },\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "prompt = \"Say hello to the class\"\n",
    "single_turn(system_prompt = system_prompt, user_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def augmented_model_call(system_prompt, user_prompt):\n",
    "    combined_prompt = f\"{system_prompt}\\n{user_prompt}\"\n",
    "    return model_call(combined_prompt)\n",
    "\n",
    "\n",
    "# This is injected by the LLM application behind the scenes\n",
    "system_prompt = '''\n",
    "You have the following functions available\n",
    " def get_weather(city: str)\n",
    "   \"\"\"Given a city returns the weather for that city\"\"\n",
    "\n",
    " If you call this function return the json [{\"city\": city}]\n",
    " otherwise respond normally\n",
    "'''\n",
    "\n",
    "# This is the user message\n",
    "user_prompt = \"Say hello to the class\"\n",
    "augmented_model_call(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\"city\": \"Sydney\"}\\n```\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"What's the weather in Sydney?\"\n",
    "augmented_model_call(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\"city\": \"Austin\"}\\n```\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"How's the Austin forecast looking today?\"\n",
    "augmented_model_call(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\"city\": \"London\"}\\n```\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"How's the London weather looking today?\"\n",
    "augmented_model_call(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Response\n",
    "Now that we can see our model is aware of the tools we need to build a framework around it to process results.\n",
    "\n",
    "We need a basic condition\n",
    "1. If there is no function call return the response to the user\n",
    "2. If there is a function call then we need to\n",
    "    a. Call the tool to get new information\n",
    "    b. Either return the response back to the user directly, or reinject it back to hte model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_response = '```json\\n{\"city\": \"Austin\"}\\n```\\n'\n",
    "\n",
    "def parse_response(model_response):\n",
    "    if tool_call := re.search(pattern, tool_response):\n",
    "        return tool_call.groups(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_response(city, weather):\n",
    "    \n",
    "    return function_response_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, let's see... it's a lovely, cloudy day in London! Itâ€™s a bit soft and gentle, perfect for a cozy day. ðŸ˜Š \\n\\nWould you like me to tell you anything more about it, like the temperature or if there's a chance of rain?\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_interaction(user_prompt):\n",
    "    model_response = augmented_model_call(system_prompt, user_prompt)\n",
    "\n",
    "    #function_call_json = parse_response(model_response)\n",
    "    function_call_json = {\"city\":\"London\"}\n",
    "\n",
    "    if not function_call_json:\n",
    "        return model_response\n",
    "\n",
    "\n",
    "    weather = get_weather(function_call_json[\"city\"])\n",
    "\n",
    "    # We have a choice here. We oculd return the weather directly to the user\n",
    "    # But for a nicer experience let's reinject in the LLM to the response\n",
    "    function_response_prompt = f\"The weather in {function_call_json['city']} is {weather}, tell me the weather nicely\"\n",
    "\n",
    "    # We already checked for weather so we don't need to go again\n",
    "    model_response = model_call(function_response_prompt)\n",
    "    return model_response\n",
    "\n",
    "chat_interaction(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Recap: What We Learned  \n",
    "\n",
    "### Function Calls are nothing magical\n",
    "* We rely on the models \"reasoning\" to understand when to make a function call or not\n",
    "\n",
    "### The model doesn't call a function itself\n",
    "* It's the framework that handles the function call\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
